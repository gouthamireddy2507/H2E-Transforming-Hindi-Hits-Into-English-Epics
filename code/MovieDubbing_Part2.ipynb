{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JrK20I32grP6"
      },
      "outputs": [],
      "source": [
        "!pip3 install -U scipy\n",
        "%cd /path/to/your/directory/\n",
        "!git clone https://github.com/jnordberg/tortoise-tts.git\n",
        "%cd tortoise-tts\n",
        "!pip3 install -r requirements.txt\n",
        "!pip3 install transformers==4.19.0 einops==0.5.0 rotary_embedding_torch==0.1.5 unidecode==1.3.5\n",
        "!python3 setup.py install\n",
        "!pip3 install -U pydub\n",
        "!pip install moviepy==2.0.0.dev2\n",
        "!pip install imageio==2.25.1\n",
        "!cat /etc/ImageMagick-6/policy.xml | sed 's/none/read,write/g'> /etc/ImageMagick-6/policy.xml\n",
        "!apt install imagemagick\n",
        "!pip install pysrt==1.1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gen09NM4hONQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import IPython\n",
        "\n",
        "from tortoise.api import TextToSpeech\n",
        "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
        "\n",
        "tts = TextToSpeech()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQgw3KeV8Yqb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import locale\n",
        "import json\n",
        "from pydub import AudioSegment\n",
        "import re\n",
        "import moviepy.editor as mp\n",
        "from moviepy.editor import *\n",
        "from moviepy.video.tools.subtitles import SubtitlesClip\n",
        "import re\n",
        "import sys\n",
        "import pysrt\n",
        "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip\n",
        "\n",
        "\n",
        "def training_voice(name, path):\n",
        "  CUSTOM_VOICE_NAME = name\n",
        "  custom_voice_folder = f\"tortoise/voices/{CUSTOM_VOICE_NAME}\" #change the path accordingly\n",
        "\n",
        "  os.makedirs(custom_voice_folder, exist_ok=True)\n",
        "\n",
        "  with open(path, 'rb') as f:\n",
        "      file_data = f.read()\n",
        "      with open(os.path.join(custom_voice_folder, f'{CUSTOM_VOICE_NAME}.wav'), 'wb') as new_file:\n",
        "          new_file.write(file_data)\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "def GettingSpeakerAudio(audio_file, txt_file):\n",
        "  def convert_to_wav(audio_segment, counter):\n",
        "      filename = f\"temp_{counter}.wav\"\n",
        "      audio_segment.export(filename, format=\"wav\")\n",
        "      return filename\n",
        "\n",
        "  def segment_audio(audio_file, txt_file):\n",
        "      audio = AudioSegment.from_file(audio_file, format=\"mp3\")\n",
        "\n",
        "      segments = {}\n",
        "      with open(txt_file, 'r', encoding='utf-8') as file:\n",
        "          txt_data = file.read().split('\\n\\n')\n",
        "          counter = 0\n",
        "          for data in txt_data:\n",
        "              data = data.split('\\n')\n",
        "              speaker = data[2].split(']')[0][1:]\n",
        "              if speaker.find('[') != -1:\n",
        "                  continue\n",
        "\n",
        "              transcript = data[2].split(']')[1]\n",
        "              start_time = data[1].split(' --> ')[0]\n",
        "              start_parts = start_time.replace(',', ':').split(':')\n",
        "              end_time = data[1].split(' --> ')[1]\n",
        "              end_parts = end_time.replace(',', ':').split(':')\n",
        "              start_ms = int(start_parts[0]) * 60 * 60 * 1000 + int(start_parts[1]) * 60 * 1000 + float(\n",
        "                  start_parts[2]) * 1000\n",
        "              end_ms = int(end_parts[0]) * 60 * 60 * 1000 + int(end_parts[1]) * 60 * 1000 + float(end_parts[2]) * 1000\n",
        "              audio_segment = audio[start_ms:end_ms + 1000]\n",
        "              filename = convert_to_wav(audio_segment, counter)\n",
        "              counter += 1\n",
        "              audio_segment.export(filename, format=\"wav\")\n",
        "              if speaker not in segments:\n",
        "                  segments[speaker] = []\n",
        "              segments[speaker].append({\"transcript\": transcript, \"filename\": filename})\n",
        "      return segments\n",
        "\n",
        "  def combine_audio_for_speaker(segments, speaker):\n",
        "      combined_audio = AudioSegment.empty()\n",
        "      for segment in segments:\n",
        "          audio_segment = AudioSegment.from_file(segment[\"filename\"], format=\"wav\")\n",
        "          combined_audio += audio_segment\n",
        "      filename = f\"/content/{speaker}.wav\" #change the path accordingly\n",
        "      combined_audio.export(filename, format=\"wav\")\n",
        "      return filename\n",
        "\n",
        "\n",
        "  audio_segments = segment_audio(audio_file, txt_file)\n",
        "\n",
        "  output_json = json.dumps({\"speakers\": audio_segments}, indent=2)\n",
        "\n",
        "  for speaker in audio_segments:\n",
        "      f = combine_audio_for_speaker(audio_segments[speaker], speaker)\n",
        "  return audio_segments\n",
        "\n",
        "def mainThing(text,count, name):\n",
        "  voice_samples, conditioning_latents = load_voice(name)\n",
        "  gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents,\n",
        "                            preset=preset)\n",
        "  torchaudio.save(f'/content/generated-{name}{count}.wav', gen.squeeze(0).cpu(), 24000) #change the path accordingly\n",
        "\n",
        "def generate_new_speech(audio_segments):\n",
        "  for speaker in audio_segments:\n",
        "      audio_path = f\"/content/{speaker}.wav\" #change the path accordingly\n",
        "      training_voice(speaker, audio_path)\n",
        "      print()\n",
        "      print(\"Generating for ...\", speaker)\n",
        "      print()\n",
        "      count =0\n",
        "      for transcript in audio_segments[speaker]:\n",
        "        text = transcript[\"transcript\"]\n",
        "        mainThing(text, count, speaker)\n",
        "        count+=1\n",
        "\n",
        "def segment_audio_new(audio_file, txt_file):\n",
        "    count_another=0\n",
        "    audio = AudioSegment.from_file(audio_file, format=\"mp3\")\n",
        "    speakers_count = {}\n",
        "    segments = {}\n",
        "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
        "        txt_data = file.read().split('\\n\\n')\n",
        "        for data in txt_data:\n",
        "            data = data.split('\\n')\n",
        "            current_speaker = data[2].split(']')[0][1:]\n",
        "            if current_speaker.find('[') != -1:\n",
        "                continue\n",
        "            transcript = data[2].split(']')[1]\n",
        "            start_time = data[1].split(' --> ')[0]\n",
        "            start_parts = start_time.replace(',', ':').split(':')\n",
        "            end_time = data[1].split(' --> ')[1]\n",
        "            end_parts = end_time.replace(',', ':').split(':')\n",
        "            start_ms = int(start_parts[0]) * 60 * 60 * 1000 + int(start_parts[1]) * 60 * 1000 + float(\n",
        "                start_parts[2]) * 1000\n",
        "            end_ms = int(end_parts[0]) * 60 * 60 * 1000 + int(end_parts[1]) * 60 * 1000 + float(end_parts[2]) * 1000\n",
        "            try:\n",
        "                speakers_count[current_speaker] += 1\n",
        "            except KeyError:\n",
        "                speakers_count[current_speaker] = 0\n",
        "\n",
        "            print(\"Speaker:\", current_speaker)\n",
        "            print(\"Count:\", speakers_count[current_speaker])\n",
        "            path_now=f\"/content/generated-{current_speaker}{speakers_count[current_speaker]}.wav\" #change the path accordingly\n",
        "            audio_now = AudioSegment.from_file(path_now, format=\"wav\")\n",
        "            audio_now.export(f\"/content/{count_another}.wav\", format=\"wav\") #change the path accordingly\n",
        "            count_another+=1\n",
        "\n",
        "\n",
        "\n",
        "def full_combine_with_music(file_path, noise_path, output_path):\n",
        "  def parse_annotations(file_path):\n",
        "      with open(file_path, 'r', encoding='utf-8') as file:\n",
        "          lines = file.readlines()\n",
        "\n",
        "      annotations = []\n",
        "\n",
        "      for line in lines:\n",
        "          match = re.match(r'(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})', line)\n",
        "          if match:\n",
        "              start_time, end_time = match.group(1), match.group(2)\n",
        "              annotations.append({'start': time_to_ms(start_time), 'end': time_to_ms(end_time), 'lines': []})\n",
        "          elif line.strip():\n",
        "              match = re.match(r'\\[([^\\]]+)\\](.*)', line)\n",
        "              if match:\n",
        "                  speaker, spoken_line = match.group(1), match.group(2).strip()\n",
        "                  annotations[-1]['lines'].append({'speaker': speaker, 'line': spoken_line})\n",
        "\n",
        "      return annotations\n",
        "\n",
        "  def time_to_ms(time_str):\n",
        "      m, s = map(int, re.split('[:,]', time_str)[1:3])\n",
        "      return m * 60000 + s * 1000\n",
        "\n",
        "  from pydub import AudioSegment\n",
        "\n",
        "  def combine_audio_with_music(annotations, music_file):\n",
        "      combined_audio = AudioSegment.silent()\n",
        "\n",
        "      for i, annotation in enumerate(annotations, start=0):\n",
        "\n",
        "            audio_file = f\"/content/{i}.wav\" #change the path accordingly\n",
        "\n",
        "            individual_segment = AudioSegment.from_file(audio_file)\n",
        "\n",
        "            gap_duration = annotation['start'] - len(combined_audio)\n",
        "\n",
        "            if gap_duration > 0:\n",
        "                gap = AudioSegment.silent(duration=gap_duration)\n",
        "                combined_audio += gap\n",
        "\n",
        "            combined_audio += individual_segment\n",
        "\n",
        "      music = AudioSegment.from_file(music_file)\n",
        "\n",
        "      combined_audio_with_music = combined_audio.overlay(music)\n",
        "\n",
        "      return combined_audio_with_music\n",
        "\n",
        "\n",
        "  def main(file_path,noise_path,output_path):\n",
        "      annotations = parse_annotations(file_path)\n",
        "\n",
        "      for i, annotation in enumerate(annotations, start=1):\n",
        "          print(f\"{i}. Start: {annotation['start']} ms, End: {annotation['end']} ms\")\n",
        "          for line in annotation['lines']:\n",
        "              print(f\"   [{line['speaker']}] {line['line']}\")\n",
        "      combined_audio = combine_audio_with_music(annotations, noise_path)\n",
        "      combined_audio.export(output_path, format=\"wav\")\n",
        "  main(file_path,noise_path,output_path)\n",
        "\n",
        "\n",
        "\n",
        "def video_srt_and_audio_add(video_path,audio_path,output_path,subtitle_path,output_srt_path):\n",
        "\n",
        "  def remove_speaker_names(srt_content):\n",
        "      pattern = re.compile(r'\\[.*?\\]')\n",
        "\n",
        "      cleaned_subtitles = [re.sub(pattern, '', subtitle) for subtitle in srt_content]\n",
        "\n",
        "      return cleaned_subtitles\n",
        "\n",
        "  def process_srt(input_path, output_path):\n",
        "      with open(input_path, 'r', encoding='utf-8') as file:\n",
        "          srt_content = file.readlines()\n",
        "      cleaned_subtitles = remove_speaker_names(srt_content)\n",
        "\n",
        "      cleaned_subtitle_content = ''.join(cleaned_subtitles)\n",
        "\n",
        "      with open(output_path, 'w', encoding='utf-8') as file:\n",
        "          file.write(cleaned_subtitle_content)\n",
        "\n",
        "  process_srt(subtitle_path, output_srt_path)\n",
        "  audio = mp.AudioFileClip(audio_path)\n",
        "  video = mp.VideoFileClip(video_path)\n",
        "\n",
        "  final_video = video.set_audio(audio)\n",
        "  final_video.write_videofile(output_path)\n",
        "\n",
        "  def time_to_seconds(time_obj):\n",
        "      return time_obj.hours * 3600 + time_obj.minutes * 60 + time_obj.seconds + time_obj.milliseconds / 1000\n",
        "\n",
        "\n",
        "  def create_subtitle_clips(subtitles, videosize,fontsize=24, font='Arial', color='yellow', debug = False):\n",
        "      subtitle_clips = []\n",
        "\n",
        "      for subtitle in subtitles:\n",
        "          start_time = time_to_seconds(subtitle.start)\n",
        "          end_time = time_to_seconds(subtitle.end)\n",
        "          duration = end_time - start_time\n",
        "\n",
        "          video_width, video_height = videosize\n",
        "\n",
        "          text_clip = TextClip(subtitle.text, fontsize=fontsize, font=font, color=color, bg_color = 'black',size=(video_width*3/4, None), method='caption').set_start(start_time).set_duration(duration)\n",
        "          subtitle_x_position = 'center'\n",
        "          subtitle_y_position = video_height* 4 / 5\n",
        "\n",
        "          text_position = (subtitle_x_position, subtitle_y_position)\n",
        "          subtitle_clips.append(text_clip.set_position(text_position))\n",
        "\n",
        "      return subtitle_clips\n",
        "\n",
        "  video = VideoFileClip(output_path)\n",
        "  subtitles = pysrt.open(output_srt_path)\n",
        "\n",
        "  begin,end= output_path.split(\".mp4\")\n",
        "  output_video_file = begin+'_subtitled'+\".mp4\"\n",
        "\n",
        "  print (\"Output file name: \",output_video_file)\n",
        "\n",
        "  subtitle_clips = create_subtitle_clips(subtitles,video.size)\n",
        "  final_video = CompositeVideoClip([video] + subtitle_clips)\n",
        "  final_video.write_videofile(output_video_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh7JC-LQH7Gx"
      },
      "outputs": [],
      "source": [
        "subtitle_path = \"path/to/your/file\"\n",
        "output_srt_path = \"path/to/your/directory\"\n",
        "vocals=\"/path/to/your/directory\"\n",
        "subtitle_file=\"/path/to/your/directory\"\n",
        "noise_file=\"/path/to/your/directory\"\n",
        "output_file=\"path/to/your/directory\"\n",
        "output_file_final=\"/path/to/your/directory\" # Final Output\n",
        "video_path = \"/path/to/your/directory/{\" # Input Video\n",
        "\n",
        "\n",
        "preset = \"ultra_fast\"\n",
        "\n",
        "audio_segments = GettingSpeakerAudio(vocals,subtitle_file)\n",
        "\n",
        "generate_new_speech(audio_segments)\n",
        "\n",
        "segment_audio_new(vocals, subtitle_file)\n",
        "\n",
        "full_combine_with_music(subtitle_file, noise_file, output_file_final)\n",
        "\n",
        "video_srt_and_audio_add(video_path,output_file,output_file_final,subtitle_path,output_srt_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}